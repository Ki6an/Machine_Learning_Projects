{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Question_Generation 1.0 .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1MpsQtcSMJUu350fEJ5eBrtnVxlL1ynqH",
      "authorship_tag": "ABX9TyMmjY/Ejno1rYB7RCOvcgH+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ki6an/Machine_Learning_Projects/blob/master/Question%20Generation/Question_Generation_1_0_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBUd059YpH1w"
      },
      "source": [
        "# pip install transformers\n",
        "\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "\n",
        "!pip install ./transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V903C6XfpSxF"
      },
      "source": [
        "from transformers import BertTokenizer, TFBertModel, tf_top_k_top_p_filtering\n",
        "import tensorflow as tf\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# bert = TFBertModel.from_pretrained(\"bert-base-uncased\") \n",
        "tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "# bert = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rwrcaioALXw"
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from keras.layers import Dense\n",
        "\n",
        "\n",
        "class QG_BERT(Model):\n",
        "    def __init__(self):\n",
        "        super(QG_BERT, self).__init__()\n",
        "        self.bert = TFBertModel.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "        self.dense = Dense(tokenizer.vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, input):\n",
        "        embedding = self.bert(input)[0][:, -1, :]\n",
        "        dense_out = self.dense(embedding)\n",
        "        # q_string, decoder_out = decoder(input_id, dense_out)\n",
        "        # input_id = decoder_out\n",
        "        # print(f'for loop  the input id is {input_id}, and decoder output is {decoder_out} and string is {q_string}')\n",
        "        return  dense_out\n",
        "\n",
        "model = QG_BERT()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u9FxcZ8vxwY"
      },
      "source": [
        "##Preprocessing the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTNUVAdQAMf5"
      },
      "source": [
        "pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1bjsYnuAMen"
      },
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"squad_v2\", split='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfBPmg2jAMVm"
      },
      "source": [
        "context = dataset['context'][0:1000]\n",
        "questions = dataset['question'][0:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMdYhc3DAMP3"
      },
      "source": [
        "# answers =  [dataset['answers'][i]['text'][0] for i in range(0,1000)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDzFXUlHnlvv"
      },
      "source": [
        "\n",
        "# np.savetxt('/content/drive/MyDrive/Machine Learning /Projects/QG/ans_list.txt', answers, delimiter=\"\\n\", fmt=\"%s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ebr-P3kovcJ"
      },
      "source": [
        "import numpy as np\n",
        "lines = np.loadtxt(\"/content/drive/MyDrive/Machine Learning /Projects/QG/ans_list.txt\",dtype=str, delimiter=\"\\n\", unpack=True)\n",
        "answers = lines.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozSYMmcmALwT"
      },
      "source": [
        "# features \n",
        "inputs = tokenizer(context, answers, padding=True, truncation=True, return_tensors=\"tf\") # gonna take (context, answers) \n",
        "# x_ids = inputs['input_ids'] \n",
        "# x_mask = inputs['attention_mask'] \n",
        "# x_train = [x_ids, x_mask]\n",
        "inputs =  inputs['input_ids']\n",
        "\n",
        "# labels\n",
        "label = tokenizer(questions, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "label =  label['input_ids']"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMM8x1OSwctR",
        "outputId": "a3d6a2b1-bf14-4ad1-cbb4-3807cf9d494b"
      },
      "source": [
        "# # Eg:\n",
        "# # z = tokenizer.decode(x_train[52]) \n",
        "# q = tokenizer.decode(y_train[52])\n",
        "# z,q\n",
        "inputs.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1000, 434])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOkFhQJLkmB8"
      },
      "source": [
        "def decoder(i, x):\n",
        "    filtered_next_token_logits = tf_top_k_top_p_filtering(x, top_k=50, top_p=1.0)\n",
        "    next_token = tf.random.categorical(filtered_next_token_logits, dtype=tf.int32, num_samples=1)\n",
        "    generated = tf.concat([i, next_token], axis=1)\n",
        "    output_id = generated.numpy().tolist()[0]\n",
        "    resulting_string = tokenizer.decode(output_id)\n",
        "    return resulting_string, generated"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0p4UgATwA2E"
      },
      "source": [
        "def compute_loss(labels, logits):\n",
        "  loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "  return loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_hat = model(x)\n",
        "        loss = compute_loss(y, y_hat)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss, y_hat\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYQh_0jt_a4E"
      },
      "source": [
        "##################\n",
        "# Begin training!#\n",
        "##################\n",
        "\n",
        "import numpy \n",
        "\n",
        "epochs = 10\n",
        "from tqdm import tqdm\n",
        "loss = 1000000.0\n",
        "history = []\n",
        "\n",
        "\n",
        "for iter in tqdm(range(epochs)):\n",
        "\n",
        "    print(f' Epoch {iter} the total loss is {loss}')\n",
        "    for i, y in enumerate(label):\n",
        "        print(f' context  {i + 1} loss is {loss}')\n",
        "\n",
        "\n",
        "        x = inputs[i]\n",
        "        x = tf.reshape(x,[1,-1])\n",
        "\n",
        "        \n",
        "        for word in tqdm(y):\n",
        "            \n",
        "            loss, y_hat = train_step(x, word)\n",
        "            text, x = decoder(x, y_hat)\n",
        "            history.append(loss.numpy().mean())\n",
        "            # m = tf.concat(m, 1)\n",
        "\n",
        "            # plt.plot(history)\n",
        "            # print(f' for {word} the loss is {loss}')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr91OCGrMU8S"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhram8lRezYf"
      },
      "source": [
        "def decoder(i, x):\n",
        "    filtered_next_token_logits = tf_top_k_top_p_filtering(x, top_k=50, top_p=1.0)\n",
        "    next_token = tf.random.categorical(filtered_next_token_logits, dtype=tf.int32, num_samples=1)\n",
        "    generated = tf.concat([i, next_token], axis=1)\n",
        "    output_id = generated.numpy().tolist()[0]\n",
        "    resulting_string = tokenizer.decode(output_id)\n",
        "    return resulting_string, generated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1_WSF0hezb3"
      },
      "source": [
        "context = 'my name is kiran'\n",
        "answers = 'kiran'\n",
        "inputs = tokenizer.encode(context, answers,padding=True, truncation=True, return_tensors=\"tf\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaLC_hv9DhEo"
      },
      "source": [
        "while True: # works as a do-while loop\n",
        "    dense_out = model(inputs)\n",
        "    q_string, decoder_out = decoder(inputs, dense_out)\n",
        "    inputs = decoder_out\n",
        "    print(f'string is {q_string}')\n",
        "    if inputs.numpy()[0][-1] == tokenizer.sep_token_id:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjPctRKhDx8x"
      },
      "source": [
        "q_string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsCVmmOFDx5t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a4LcVLBDx0K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF4QXLcl9BAF"
      },
      "source": [
        "# batch_size = 64\n",
        "\n",
        "\n",
        "# def get_batch(x, batch_size):\n",
        "#     x = tf.data.Dataset.from_tensor_slices(x)\n",
        "#     x = x.batch(batch_size)\n",
        "#     return x\n",
        "\n",
        "# # x_train = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "# # x_train = x_train.batch(batch_size)\n",
        "# # x_train\n",
        "\n",
        "# x_batch = get_batch(x_train, batch_size)\n",
        "# y_batch = get_batch(y_train, batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZezlGUYGoVP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}