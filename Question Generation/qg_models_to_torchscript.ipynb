{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qg_models_to_torchscript.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ki6an/Machine_Learning_Projects/blob/master/Question%20Generation/qg_models_to_torchscript.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXmTQOB0oZS1",
        "outputId": "f8332870-0f06-43fe-dc68-366be52a9ca7"
      },
      "source": [
        "!pip install -U transformers==4.0.0\n",
        "!python -m nltk.downloader punkt\n",
        "# !pip install onnx\n",
        "!pip install sentencepiece==0.1.94"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: transformers==4.0.0 in /usr/local/lib/python3.6/dist-packages (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.0.0) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.0.0) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.0.0) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.0.0) (0.17.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.0) (2020.11.8)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.0.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.0.0) (2.4.7)\n",
            "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: sentencepiece==0.1.94 in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKtjtRx5QqOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b33460-a2f5-473b-8c25-e02d2edec914"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from transformers import top_k_top_p_filtering\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('valhalla/t5-base-qg-hl')\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained('valhalla/t5-base-qg-hl', torchscript = True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at valhalla/t5-base-qg-hl were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight']\n",
            "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkGs5K1FQqL6"
      },
      "source": [
        "t_input =  'Python is a programming language. It is developed by <hl> Guido Van Rossum <hl>. </s>'\n",
        "label = 'Who developed Python?' # decoder input \n",
        "\n",
        "token = tokenizer(t_input,\n",
        "                  padding=True, \n",
        "                  truncation=True,\n",
        "                  max_length=64,\n",
        "                  add_special_tokens=True, \n",
        "                  pad_to_max_length = 64,\n",
        "                  return_tensors='pt')\n",
        "\n",
        "# attention_mask = input_ids.ne(model.config.pad_token_id).long()\n",
        "input_ids = token['input_ids']\n",
        "attention_mask = token['attention_mask']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53SyAUDGW8Ki"
      },
      "source": [
        "# decoder inputs \n",
        "token_for_decoder  = tokenizer(label,\n",
        "                              padding=True, \n",
        "                              truncation=True,\n",
        "                              max_length=64,\n",
        "                              add_special_tokens=True, \n",
        "                              pad_to_max_length = 64,\n",
        "                              return_tensors='pt')\n",
        "\n",
        "\n",
        "decoder_input_ids = token_for_decoder['input_ids']\n",
        "decoder_attention_mask = token_for_decoder['attention_mask']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIyE9vapTxz2"
      },
      "source": [
        "o = model.generate(input_ids = input_ids,\n",
        "                   attention_mask = attention_mask, \n",
        "                   max_length=32, \n",
        "                   num_beams=4)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrXT-ny9jgQa",
        "outputId": "6fc50ee1-cd52-4287-f57c-d7942736273f"
      },
      "source": [
        "o.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5z5AWCQQUQtf",
        "outputId": "c5500b7e-e1a1-4533-d992-ae48de50cb3e"
      },
      "source": [
        "[tokenizer.decode(ids, skip_special_tokens=True) for ids in o]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Who developed Python?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdXsSaiAdfPd"
      },
      "source": [
        "input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJr15_srgMwY"
      },
      "source": [
        "q_model = torch.quantization.convert(model)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScX9A8zwcEK6",
        "outputId": "72553840-fc72-453c-d592-6e9e4e2ae118"
      },
      "source": [
        "traced_model = torch.jit.trace(q_model, (input_ids, attention_mask, decoder_input_ids, decoder_attention_mask))\n",
        "torch.jit.save(traced_model, \"proper_qg_model.pt\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py:244: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if causal_mask.shape[1] < attention_mask.shape[1]:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYPZRLLmfyYn"
      },
      "source": [
        "import torch.utils.mobile_optimizer as mobile_optimizer\n",
        "\n",
        "opt_model = mobile_optimizer.optimize_for_mobile(traced_model)\n",
        "\n",
        "torch.jit.save(opt_model, 'mobile_qg.pt')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEaHdc7ufyVi"
      },
      "source": [
        ""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0sYq07afss4"
      },
      "source": [
        "loaded_model = torch.jit.load('proper_qg_model.pt')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d97MGdjfsqf"
      },
      "source": [
        "loaded_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvMTACBRjJZD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE0TqQxcgIrf"
      },
      "source": [
        "out = loaded_model(input_ids, attention_mask, decoder_input_ids, decoder_attention_mask)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrtYKb0jgIk6"
      },
      "source": [
        "out[0] # <---- equivalent to logits "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g83tniG31uy"
      },
      "source": [
        "ox = model(input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, return_dict=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Irmg9H2B5lc"
      },
      "source": [
        "# ox.encoder_last_hidden_state.shape"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WsuIcbpnZTx",
        "outputId": "67531e9f-4753-4d29-90b2-e548a2813bc0"
      },
      "source": [
        "logits = ox.logits\n",
        "logits"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-24.5739,  -5.6539, -12.7627,  ..., -12.7520, -24.0905, -47.1493],\n",
              "         [-29.1001, -10.3235, -14.7260,  ..., -15.0751, -25.0520, -46.0399],\n",
              "         [-31.2355,  -7.1591, -13.7183,  ..., -14.3682, -26.8118, -49.2356],\n",
              "         [-35.4251,  -5.6868, -15.0985,  ..., -16.6765, -31.1287, -51.3213],\n",
              "         [-31.2978,  -5.6199, -15.4944,  ..., -16.0537, -27.3588, -49.4906]]],\n",
              "       grad_fn=<UnsafeViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJydDaE15pkG"
      },
      "source": [
        "def ids_decoder_by_top_k(logits):\n",
        "  output_text = []\n",
        "  for i, _ in enumerate(logits[0]):   \n",
        "    _input = torch.unsqueeze(logits[0][i], dim=1).t()\n",
        "    filtered_next_token_logits = top_k_top_p_filtering(_input, top_k=50, top_p=1.0)\n",
        "    probs = F.softmax(filtered_next_token_logits, dim=-1)\n",
        "    token = torch.multinomial(probs, num_samples=1)\n",
        "    text = tokenizer.decode(token[0], skip_special_tokens=True) \n",
        "    output_text.append(text)\n",
        "  return output_text"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp4JT8c78x3r",
        "outputId": "482fbb60-a4b0-41f1-9c4b-6291cb30a91e"
      },
      "source": [
        "ids_decoder_by_top_k(logits)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'Python', '?', '', '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDZeZxire0tw",
        "outputId": "370f0fc3-d694-4edb-dde4-4f7d893791ba"
      },
      "source": [
        "logits[0]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-24.5739,  -5.6539, -12.7627,  ..., -12.7520, -24.0905, -47.1493],\n",
              "        [-29.1001, -10.3235, -14.7260,  ..., -15.0751, -25.0520, -46.0399],\n",
              "        [-31.2355,  -7.1591, -13.7183,  ..., -14.3682, -26.8118, -49.2356],\n",
              "        [-35.4251,  -5.6868, -15.0985,  ..., -16.6765, -31.1287, -51.3213],\n",
              "        [-31.2978,  -5.6199, -15.4944,  ..., -16.0537, -27.3588, -49.4906]],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4koWsgWkhb8"
      },
      "source": [
        "x = 'there are <hl> 8 planets <hl> in the solar system. </s>'\n",
        "\n",
        "token = tokenizer(x,\n",
        "                  padding=True, \n",
        "                  truncation=True,\n",
        "                  max_length=64,\n",
        "                  add_special_tokens=True, \n",
        "                  pad_to_max_length = 64,\n",
        "                  return_tensors='pt')\n",
        "\n",
        "# attention_mask = input_ids.ne(model.config.pad_token_id).long()\n",
        "input_ids = token['input_ids']\n",
        "attention_mask = token['attention_mask']\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScC9c9j6n-CS"
      },
      "source": [
        "y = model(input_ids, attention_mask,) #todo )"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFDiTipNooyy",
        "outputId": "2533afb3-4201-4051-8c07-6322d2a8a895"
      },
      "source": [
        "l = y[0][0]\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([14, 32101])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOI032VpfSjR"
      },
      "source": [
        "data = []\n",
        "for i,_ in enumerate(l):\n",
        "  probs = F.softmax(l[i], dim=-1)\n",
        "  data.append(probs)\n",
        "\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP414j6GndnS"
      },
      "source": [
        "# model.generate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip21FMST83Ra"
      },
      "source": [
        "# does not work find a different method\n",
        "\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        " \n",
        "\n",
        "def beam_search_decoder(data, k):\n",
        "  sequences = [[list(), 0.0]]\n",
        "  # walk over each step in sequence\n",
        "  for row in data:\n",
        "      all_candidates = list()\n",
        "      # expand each current candidate\n",
        "      for i in range(len(sequences)):\n",
        "          seq, score = sequences[i]\n",
        "          for j in range(len(row)):\n",
        "              candidate = [seq + [j], score - log(row[j])]\n",
        "              all_candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "      ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "      # select k best\n",
        "      sequences = ordered[:k]\n",
        "  return sequences"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiOqVZy583PI",
        "outputId": "f712a30f-f71c-4dc8-a7ba-5e4e404e18d3"
      },
      "source": [
        "beam_search_decoder(data, 3)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[3, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 4.054392174941449],\n",
              " [[3, 1, 58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 4.194602372503295],\n",
              " [[3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 4.930723054763933]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o1vByN4S83ME",
        "outputId": "aa1fea5c-dd4c-4e78-8841-3baf6fd1dcef"
      },
      "source": [
        " ids = [3, 1, 58, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        " text = tokenizer.decode(ids, skip_special_tokens=True) \n",
        " text"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut1uqdM5mQfP"
      },
      "source": [
        "# the converted torchscript model and mobile models were of same size, and there was no reduction of the model size. \n",
        "# the generated models were slightly bigger than the actual model (pretrain model used for converting).\n",
        "# the generate() method is not produced in the converted torchscript model, had to find out a way to output the logits.\n",
        "# decoder_input_ids should always be given model(). or its gonna show error. Which makes the process more complex...\n",
        "# still need to look at the source code of t5 and how it is trained on qg. (understanding those will be the key to solving these problems \n",
        "# i guess)\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMMsUI-XgIfc"
      },
      "source": [
        "# import numpy as np\n",
        "\n",
        "# y = np.around(y_cap.detach().numpy(), 2) # detach() is neccessy, because the torch tensors trace the oparation applied on them.\n",
        "# # detach is basically detaching the tensor from the tensor graph. \n",
        "# y "
      ],
      "execution_count": 34,
      "outputs": []
    }
  ]
}